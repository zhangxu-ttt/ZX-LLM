# SFT（监督微调）配置文件

# Tokenizer路径
tokenizer_path: "tokenizer/minimind"

# 模型配置
model:
  vocab_size: 6400
  n_layers: 24
  d_model: 1024
  q_head: 16
  kv_head: 4
  d_ff: 4096
  dropout_p: 0.1
  max_seq_length: 1024
  rope_theta: 10000.0

  pretrained_model_path: output/pretrain_hf/

# 数据配置
data:
  train_data_path: ["data/sft_512.jsonl"]
  max_length: 1024
  num_workers: 16              # 增加到CPU核心数的1/2到2/3
  prefetch_factor: 4           # 增加预取buffer，从2改到4
  persistent_workers: true     # 添加这行：保持worker进程不重启

# 训练配置
training:
  num_epochs: 2
  learning_rate: 1.0e-5            # SFT通常使用较小的学习率
  weight_decay: 0.01
  warmup_steps: 100                # SFT通常使用较少的warmup
  max_steps: -1
  
  # 批次大小配置
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 4
  
  # 日志和保存
  eval_steps: 200
  save_steps: 500
  logging_steps: 10
  
  # 优化器配置
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-8
  
  # 其他配置
  gradient_checkpointing: true
  max_grad_norm: 1.0
  seed: 42
  
# DeepSpeed配置
deepspeed:
  config_path: "config/deepspeed_config_stage2_sft.json"
  
# 输出配置
output:
  output_dir: "output/sft"
  save_total_limit: 5
  resume_from_checkpoint: null
  output_hf_dir: "output/sft_hf"
  
# WandB配置
wandb:
  enabled: true
  project: "zx-llm-sft"
  run_name: "sft_run_001"
  entity: null

